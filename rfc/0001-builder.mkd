# RFC 0001 - Builder Protocol

## Context

FastStore uses Gatsby to statically generate stores' pages. This RFC details how this builder would work within this scope.

When building a Gatsby site there must be a function that decides what pages should be statically generated by the builder. For VTEX Stores' this will amount to a non-trivial number of GraphQL queries.

For this RFC there's a clear distinction between **static, non-contextual details** of a product (such as description, image, collections that it belongs to, category) and **dynamic, contextual details** (such as availability in a given zone or the price for a specific customer).

For the purpose of this RFC builds can be triggered by some distinct events:
* **Deploy Preview** - when a code change gets pushed to the repo (via push to Branch, Tag, Master or Pull Request);
* **Store Data Change** - when a change in product's details happen.
* **CMS Page** - when a page is created or updated.

There's a router in place that knows how to translate requests coming in (https) to the built store assets and respond to the request.

## Proposal

1. There exists a system that receives pixel-like notifications about product usage in the store pages. This system updates a graph of pages that link to the product for each version.
1. There is a Content-Aware-Storage (CAS) that works with the router to store trees that link an URL path to a static asset. This CAS stores separate trees for each version of the site (master, branches, tags, PRs). It must support incremental updates of these trees.
1. Upon product change events, the builder can query the graph of pages and decide which URLs to statically generate and update the CAS with.
1. There is a way to lazily optimize a store that uses navigation data for static site generation, only touching a small portion of pages per build and updating the tree of paths for specific versions.
1. CMS Page Changes trigger events in the building pipeline that only build said page and update the builder with the results for a specific version of the site.

### Store Data Change Building

When a product gets updated the store's pages where the product appears must be regenerated, such as:

* The product details page (PDP) that shows this information;
* Category pages where the product appears;
* Home page if the product is in a shelf there;
* CMS landing pages where the product appears.

For the builder to generate only the required pages when triggered by Store Data Change, there must exist a graph that informs this decision.

#### The Store Pages Graph

This graph describes the relationship between these entities:

* Products (via ProductID);
* Product Details Page (shows product details via ProductID);
* Category Page that product appears in (via ProductID);
* Any other page that product appears in (via ProductID).

Let's assume a Knife Set with ID `knife-set` for a store. A directed acyclic graph for this product would look like this:

* `knife-set` ==belongs to==> `/knife-set/p` product page
* `knife-set` <==represents== `/knife-set/p` product page
* `knife-set` ==appears in==> `/knives` category page
* `knife-set` <==contains== `/knives` category page
* `knife-set` ==appears in==> `/tramontina` category page
* `knife-set` <==contains== `/tramontina` category page
* `knife-set` ==appears in==> `/` home page
* `knife-set` <==contains== `/` home page
* `knife-set` ==appears in==> `/summer-sale` special page
* `knife-set` <==contains== `/summer-sale` special page

With such a graph when an event comes in that a product with ID `knife-set` was changed, figuring out the pages to generate becomes trivial and fast.

#### Building the Graph

There are two ways to build the product graph: `eager` or `lazy`. This RFC proposes that building it eagerly is prohibitively expensive.

As there are stores with more than a million items in their catalog (and it is not the goal of the platform to constrain catalog size), this system should be:

* **Incremental** - the graph of nodes is built upon usage of the store;
* **Self-Healing** - When a product should not be in a given page anymore, the system should take care of that;
* **Fast** - Querying for URLs for a product in the graph should be fast as not to hinder builder performance.

Building the graph should be done in two ways:

1. When an event for product change kicks in, the graph relationships for well-known pages is updated (PDP and Catalog pages);
2. There is a pixel that notifies the router system of products that appear in a page.

Let's take the example of the `knife-set` above:

* In the `/knife-set/p` PDP there will be a pixel passing in `["knife-set"]`;
* In the `/knives` Category page there will be a pixel passing in `["knife-set", "kitchen-knife", ...]` list of products that appear in the page;
* In the `/` Home page there will be a pixel passing in `["knife-set", "kitchen-pan", "air-fryer", ...]` list of products that appear in the page.

This way the relationship between pages and products can be kept up-to-date, in a lazy manner.

#### Graph Edge Cases

* **What happens in the beginning where no graph data is present for a branch?**<br />The page won't update custom pages where the product should appear, as the system is not aware of the presence of it in these pages. Well-known pages such as PDP and Category will be updated. Once navigation data comes in, these pages will be updated as the relationship will now be in the graph.
* **Once a page gets generated with products A, B, C and D, what happens if E must be in it due to product change?**<br />What happens depends on what page it is: a **well-known page** or a **custom page**.<br />For well-known pages (PDP and Category) the product update will also update the graph, so everything should work properly.<br />For custom pages one of two things could happen: there is a dynamic part of the page that ends up showing E and that way, the graph will be whole and self-healing. The other possibility is that E should be in the static part of the page and it won't be updated until either the store gets a code change or another product in the page changes.
* **What happens if a product is deleted or out of stock and must not be shown in the store?** - It can be removed from the graph and the system rebuilds the pages it referred to previously.

#### Product Change Build Lifecycle

This is a [sequence diagram](https://bramp.github.io/js-sequence-diagrams/) for what happens when a product change event or a pixel page event comes to the build system:

[//]: # (
Source of the diagram SVG:
participant Platform
participant Builder
participant Graph
participant Router
Note right of Platform: EVENT: Page product pixel
Platform -> Graph: Update graph relationships for specific page
Note right of Platform: EVENT: Product change
Platform -> Graph: Update graph for well-known pages
Platform -> Builder: Notifies product change
Builder -> Graph: Get page URLs by product ID
Builder -> Router: Build and store new pages
)

![](0001-build-sequence.png)

### CAS & Router

If the builder stores static pages in separate buckets for each active version (deploy previews, branches, tags) there would be a lot of duplicated storage, thus making it harder to build incrementally.

Ideally what the system would do is store each page as a hash of its content and then keep track of `URL => hash` for each version.

Let's check an example:

* The builder runs and creates `/: ct6fCvsC3aPBYKVpe8LbwykGItY=`, `/knife-set/p: onBH2M/dlpaxz8E7w2A3kvnJdv4=`, `/knives: cYesU/7QAqS3suoDvqGj4G6UEyM=` and so on.
* It then uploads these to the CAS;
* Last it updates the required active versions (master, branches, tags, etc) to point to these new generated hashes.

With this system in place, the builder can verify if the file exists in the CAS before uploading, thus saving networking and time.

The router can easily translate between requested path and hash (if the hash exists), thus serving the proper static asset from the CAS:

* Request gets in from the CDN;
* Based on request data (domain & subdomain, headers, cookies, ...) router decides on the version to use (master, branch, tag, deploy preview, ...);
* Router queries the CAS for specific path and version and gets a full absolute URL to asset;
* It then reverse proxy it to the CDN.

The diagram is as follows:

[//]: # (
Source of the diagram SVG:
participant CDN
participant Builder
participant CAS
participant Router
Note right of Builder: EVENT: build finished
Builder -> CAS: Upload updated files using content hash as filename
Builder -> CAS: For each active version update path to hash relationships
Note right of CDN: EVENT: customer requests page
CDN -> Router: Sends request to router
Router -> CAS: Request asset for path and version
CAS -> Router: Responds with full path for static asset
Router -> CDN: Reverse proxy static asset
)

![](0001-router-sequence.png)

### Deploy Preview Building

Deploy previews pose a challenge if there is an assumption that a critical mass of generated files must be present for the site to be fast.

Even though building the pages takes time, the biggest challenge does not lie in that area. The issue is on **data retrieval**.

Let's assume this will be done eagerly and discuss the overall process:

* An event kicks in that requires a build (a new branch was pushed and must be built and served);
* The builder gets notified and now must decide on what pages to build. Let's assume for this part of the discussion that this can be easily retrieved and the builder now knows the 5000 pages it must build.
* It must now issue GraphQL queries for all the 5000 pages in VTEX's platform.

And here lies the problem: **that's a lot of data fetching!**

This RFC proposes a different approach: the builder will **lazily build the site**, using the same approach for product events.

**Lazy Build Proposal**:

* Build all known pages that do not vary by product (home page) or indicated in a file in the repository (list of URLs the user wants to be generated);
* Dynamically serve all the other pages;
* Rely on the pixels to build the graph of product -> page relationships;
* Queue build of all pages related to products that come in via pixels.

The reason for a build queue is so the builder can group these product page requests into a single unit and optimize all of them together. This reduces the overhead of the build process.

This way the store is lazily improving speed using navigation data as source.

#### Deploy Preview Edge Cases

* **What happens when new code gets pushed to a version?**<br/>
    The event simply deletes the tree for that version from the router and the process starts again as if the branch is a brand new one (check the lazy build proposal above).
* **What if there are 1000 new product graph changes in a single instant? Wouldn't that trigger a big build?**<br />
    That's precisely why the system sports a queue. If there are too many URLs to generate at once, it is easy to partition into a maximum number of items that can be built in batches. It also allows the system to do a little debouncing when requests for building come in (build only in intervals of minimum of X minutes).
* **This means that when users are using Deploy Preview the site won't be as fast as possible, right?**<br />
    The pages we'll generate will be really fast, so the home page and any other pages specified to be statically generated will be fast. If the store owner already knows pages that should be fast, they can easily specify these in the repository.<br />
    The counter to the fact that not all pages will be blazing fast is that getting a deploy preview for new branches/PRs/tags/etc. shouldn't take long as there's no need to generate a large number of pages.

### CMS Page Event

This is the simpler of the cases, since it just builds up on the previous scenarios. All that needs to be done is:

* Build the specific page if it has been created or updated;
* Update it in CAS if needed;
* Update CAS `path -> hash` for all appropriate versions (remove if deleted).
